{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# FC Layer Class\n",
    "class FC:\n",
    "    def __init__(self, n_input, n_output, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Initialize a fully connected layer.\n",
    "        :param n_input: Number of input features (previous layer size)\n",
    "        :param n_output: Number of neurons in the current layer\n",
    "        :param initializer: A function to initialize weights and biases\n",
    "        :param optimizer: An optimizer for updating the weights\n",
    "        \"\"\"\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.initializer = initializer  # Initialize weights and biases\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W, self.B = self.initializer.initialize(n_input, n_output)\n",
    "        \n",
    "        # Store input and output for backpropagation\n",
    "        self.X = None  # To store the input during forward pass\n",
    "        self.Z = None  # To store the output (activations) during forward pass\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward pass of the fully connected layer.\n",
    "        :param X: Input data (shape: [batch_size, n_input])\n",
    "        :return: Output of the layer (shape: [batch_size, n_output])\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.B  # Linear transformation: Z = XW + B\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Perform backward pass of the fully connected layer.\n",
    "        :param dA: Gradients of the loss with respect to the output of this layer (shape: [batch_size, n_output])\n",
    "        :return: Gradients with respect to the input (shape: [batch_size, n_input])\n",
    "        \"\"\"\n",
    "        # Compute gradients for W, B, and input X\n",
    "        dW = np.dot(self.X.T, dA)  # Gradient of the loss with respect to W\n",
    "        dB = np.sum(dA, axis=0, keepdims=True)  # Gradient of the loss with respect to B\n",
    "        dX = np.dot(dA, self.W.T)  # Gradient of the loss with respect to the input X\n",
    "\n",
    "        # Update weights and biases using the optimizer\n",
    "        self.optimizer.update(self, dW, dB)\n",
    "\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma  # Standard deviation for Gaussian distribution\n",
    "    \n",
    "    def initialize(self, n_input, n_output):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases.\n",
    "        :param n_input: Number of input features (size of previous layer)\n",
    "        :param n_output: Number of output neurons (size of current layer)\n",
    "        :return: Initialized weights and biases\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_input, n_output) * self.sigma  # Gaussian initialization\n",
    "        B = np.zeros((1, n_output))  # Bias initialized to zero\n",
    "        return W, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and use FC layers\n",
    "n_features = 784  # Example input size (e.g., 28x28 image flattened)\n",
    "n_nodes1 = 64  # Number of neurons in the first hidden layer\n",
    "n_nodes2 = 32  # Number of neurons in the second hidden layer\n",
    "n_output = 10  # Number of output classes (e.g., for MNIST)\n",
    "\n",
    "# Create an instance of the optimizer and initializer\n",
    "optimizer = SGD(lr=0.01)\n",
    "initializer = SimpleInitializer(sigma=0.01)\n",
    "\n",
    "# Create FC layers\n",
    "FC1 = FC(n_features, n_nodes1, initializer, optimizer)\n",
    "activation1 = Tanh()  # Example activation function\n",
    "\n",
    "FC2 = FC(n_nodes1, n_nodes2, initializer, optimizer)\n",
    "activation2 = Tanh()\n",
    "\n",
    "FC3 = FC(n_nodes2, n_output, initializer, optimizer)\n",
    "activation3 = Softmax()  # Softmax for the output layer (classification)\n",
    "\n",
    "# Forward pass\n",
    "X_train = np.random.randn(100, n_features)  # Example input batch (100 samples)\n",
    "A1 = FC1.forward(X_train)\n",
    "Z1 = activation1.forward(A1)\n",
    "\n",
    "A2 = FC2.forward(Z1)\n",
    "Z2 = activation2.forward(A2)\n",
    "\n",
    "A3 = FC3.forward(Z2)\n",
    "Z3 = activation3.forward(A3)\n",
    "\n",
    "# Backward pass\n",
    "Y_train = np.random.randn(100, n_output)  # Example one-hot encoded labels\n",
    "dA3 = activation3.backward(Z3, Y_train)\n",
    "dZ2 = FC3.backward(dA3)\n",
    "dA2 = activation2.backward(dZ2)\n",
    "dZ1 = FC2.backward(dA2)\n",
    "dA1 = activation1.backward(dZ1)\n",
    "dZ0 = FC1.backward(dA1)  # Final gradient (for backpropagation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleInitializer class with the given standard deviation (sigma).\n",
    "        :param sigma: Standard deviation for Gaussian weight initialization.\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def initialize(self, n_input, n_output):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases for the given layer size.\n",
    "        :param n_input: Number of input neurons (size of the previous layer).\n",
    "        :param n_output: Number of output neurons (size of the current layer).\n",
    "        :return: Initialized weights (W) and biases (B).\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_input, n_output) * self.sigma  # Gaussian initialization for weights\n",
    "        B = np.zeros((1, n_output))  # Bias initialized to zero\n",
    "        return W, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, n_input, n_output, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Initialize a fully connected layer.\n",
    "        :param n_input: Number of input neurons (size of the previous layer)\n",
    "        :param n_output: Number of output neurons (size of the current layer)\n",
    "        :param initializer: Instance of an initializer class (e.g., SimpleInitializer)\n",
    "        :param optimizer: An optimizer for updating the weights (e.g., SGD)\n",
    "        \"\"\"\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.initializer = initializer  # The initializer instance passed to the constructor\n",
    "        self.optimizer = optimizer  # The optimizer instance (e.g., SGD)\n",
    "        \n",
    "        # Initialize weights and biases using the initializer\n",
    "        self.W, self.B = self.initializer.initialize(n_input, n_output)\n",
    "        \n",
    "        # Store input and output for backpropagation\n",
    "        self.X = None  # To store the input during the forward pass\n",
    "        self.Z = None  # To store the output (activations) during the forward pass\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the fully connected layer.\n",
    "        :param X: Input data (shape: [batch_size, n_input])\n",
    "        :return: Output of the layer (shape: [batch_size, n_output])\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.B  # Linear transformation: Z = XW + B\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Perform the backward pass of the fully connected layer.\n",
    "        :param dA: Gradients of the loss with respect to the output of this layer (shape: [batch_size, n_output])\n",
    "        :return: Gradients with respect to the input (shape: [batch_size, n_input])\n",
    "        \"\"\"\n",
    "        # Compute gradients for W, B, and input X\n",
    "        dW = np.dot(self.X.T, dA)  # Gradient of the loss with respect to W\n",
    "        dB = np.sum(dA, axis=0, keepdims=True)  # Gradient of the loss with respect to B\n",
    "        dX = np.dot(dA, self.W.T)  # Gradient of the loss with respect to the input X\n",
    "\n",
    "        # Update weights and biases using the optimizer\n",
    "        self.optimizer.update(self, dW, dB)\n",
    "\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "n_features = 784  # Example input size (e.g., 28x28 image flattened)\n",
    "n_nodes1 = 64  # Number of neurons in the first hidden layer\n",
    "n_nodes2 = 32  # Number of neurons in the second hidden layer\n",
    "n_output = 10  # Number of output classes (e.g., for MNIST)\n",
    "\n",
    "# Create an instance of the SimpleInitializer with a given standard deviation\n",
    "initializer = SimpleInitializer(sigma=0.01)\n",
    "\n",
    "# Create an optimizer instance (e.g., SGD with learning rate 0.01)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "# Create fully connected layers, passing the initializer to each layer\n",
    "FC1 = FC(n_features, n_nodes1, initializer, optimizer)\n",
    "activation1 = Tanh()  # Example activation function (Tanh)\n",
    "\n",
    "FC2 = FC(n_nodes1, n_nodes2, initializer, optimizer)\n",
    "activation2 = Tanh()\n",
    "\n",
    "FC3 = FC(n_nodes2, n_output, initializer, optimizer)\n",
    "activation3 = Softmax()  # Softmax for the output layer (classification)\n",
    "\n",
    "# Forward pass with dummy input\n",
    "X_train = np.random.randn(100, n_features)  # Example input batch (100 samples)\n",
    "A1 = FC1.forward(X_train)\n",
    "Z1 = activation1.forward(A1)\n",
    "\n",
    "A2 = FC2.forward(Z1)\n",
    "Z2 = activation2.forward(A2)\n",
    "\n",
    "A3 = FC3.forward(Z2)\n",
    "Z3 = activation3.forward(A3)\n",
    "\n",
    "# Backward pass with dummy labels (Y_train would be one-hot encoded)\n",
    "Y_train = np.random.randn(100, n_output)  # Example one-hot encoded labels\n",
    "dA3 = activation3.backward(Z3, Y_train)\n",
    "dZ2 = FC3.backward(dA3)\n",
    "dA2 = activation2.backward(dZ2)\n",
    "dZ1 = FC2.backward(dA2)\n",
    "dA1 = activation1.backward(dZ1)\n",
    "dZ0 = FC1.backward(dA1)  # Final gradient (for backpropagation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        \"\"\"\n",
    "        Initialize the Stochastic Gradient Descent (SGD) optimizer.\n",
    "        :param lr: Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dW, dB):\n",
    "        \"\"\"\n",
    "        Update the weights and biases using the gradients computed during backpropagation.\n",
    "        :param layer: The layer whose weights are to be updated (FC layer).\n",
    "        :param dW: Gradient of the loss with respect to the weights.\n",
    "        :param dB: Gradient of the loss with respect to the biases.\n",
    "        \"\"\"\n",
    "        # Update the weights and biases\n",
    "        layer.W -= self.lr * dW\n",
    "        layer.B -= self.lr * dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, n_input, n_output, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Initialize a fully connected layer.\n",
    "        :param n_input: Number of input neurons (size of the previous layer)\n",
    "        :param n_output: Number of output neurons (size of the current layer)\n",
    "        :param initializer: Instance of an initializer class (e.g., SimpleInitializer)\n",
    "        :param optimizer: An optimizer for updating the weights (e.g., SGD)\n",
    "        \"\"\"\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.initializer = initializer  # The initializer instance passed to the constructor\n",
    "        self.optimizer = optimizer  # The optimizer instance (e.g., SGD)\n",
    "        \n",
    "        # Initialize weights and biases using the initializer\n",
    "        self.W, self.B = self.initializer.initialize(n_input, n_output)\n",
    "        \n",
    "        # Store input and output for backpropagation\n",
    "        self.X = None  # To store the input during the forward pass\n",
    "        self.Z = None  # To store the output (activations) during the forward pass\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the fully connected layer.\n",
    "        :param X: Input data (shape: [batch_size, n_input])\n",
    "        :return: Output of the layer (shape: [batch_size, n_output])\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.B  # Linear transformation: Z = XW + B\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Perform the backward pass of the fully connected layer.\n",
    "        :param dA: Gradients of the loss with respect to the output of this layer (shape: [batch_size, n_output])\n",
    "        :return: Gradients with respect to the input (shape: [batch_size, n_input])\n",
    "        \"\"\"\n",
    "        # Compute gradients for W, B, and input X\n",
    "        dW = np.dot(self.X.T, dA)  # Gradient of the loss with respect to W\n",
    "        dB = np.sum(dA, axis=0, keepdims=True)  # Gradient of the loss with respect to B\n",
    "        dX = np.dot(dA, self.W.T)  # Gradient of the loss with respect to the input X\n",
    "\n",
    "        # Update weights and biases using the optimizer\n",
    "        self.optimizer.update(self, dW, dB)\n",
    "\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initialization\n",
    "n_features = 784  # Input size (e.g., for MNIST dataset)\n",
    "n_nodes1 = 64  # Number of neurons in the first hidden layer\n",
    "n_nodes2 = 32  # Number of neurons in the second hidden layer\n",
    "n_output = 10  # Number of output classes (e.g., for MNIST)\n",
    "\n",
    "# Create an instance of the SimpleInitializer with a given standard deviation\n",
    "initializer = SimpleInitializer(sigma=0.01)\n",
    "\n",
    "# Create an instance of the SGD optimizer with a learning rate\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "# Create fully connected layers, passing the initializer and optimizer to each layer\n",
    "FC1 = FC(n_features, n_nodes1, initializer, optimizer)\n",
    "activation1 = Tanh()  # Example activation function (Tanh)\n",
    "\n",
    "FC2 = FC(n_nodes1, n_nodes2, initializer, optimizer)\n",
    "activation2 = Tanh()\n",
    "\n",
    "FC3 = FC(n_nodes2, n_output, initializer, optimizer)\n",
    "activation3 = Softmax()  # Softmax for the output layer (classification)\n",
    "\n",
    "# Forward pass with dummy input\n",
    "X_train = np.random.randn(100, n_features)  # Example input batch (100 samples)\n",
    "A1 = FC1.forward(X_train)\n",
    "Z1 = activation1.forward(A1)\n",
    "\n",
    "A2 = FC2.forward(Z1)\n",
    "Z2 = activation2.forward(A2)\n",
    "\n",
    "A3 = FC3.forward(Z2)\n",
    "Z3 = activation3.forward(A3)\n",
    "\n",
    "# Backward pass with dummy labels (Y_train would be one-hot encoded)\n",
    "Y_train = np.random.randn(100, n_output)  # Example one-hot encoded labels\n",
    "dA3 = activation3.backward(Z3, Y_train)\n",
    "dZ2 = FC3.backward(dA3)\n",
    "dA2 = activation2.backward(dZ2)\n",
    "dZ1 = FC2.backward(dA2)\n",
    "dA1 = activation1.backward(dZ1)\n",
    "dZ0 = FC1.backward(dA1)  # Final gradient (for backpropagation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Compute the forward pass for the Tanh activation.\"\"\"\n",
    "        self.Z = Z\n",
    "        return np.tanh(Z)\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"Compute the backward pass for the Tanh activation.\"\"\"\n",
    "        return dA * (1 - np.tanh(self.Z)**2)  # Derivative of Tanh\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Compute the forward pass for the ReLU activation.\"\"\"\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"Compute the backward pass for the ReLU activation.\"\"\"\n",
    "        return dA * (self.Z > 0)  # Derivative of ReLU\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Compute the forward pass for the Softmax activation.\"\"\"\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Stability trick\n",
    "        self.A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)  # Softmax\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA, Y):\n",
    "        \"\"\"Compute the backward pass for the Softmax activation (with cross-entropy).\"\"\"\n",
    "        # Simplified backpropagation for Softmax + Cross-Entropy\n",
    "        m = Y.shape[0]  # Number of samples\n",
    "        dZ = self.A - Y  # Simplified derivative (Softmax + Cross-Entropy)\n",
    "        return dZ / m  # Gradient w.r.t the input to the Softmax layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, n_input, n_output, initializer, optimizer, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize a fully connected layer.\n",
    "        :param n_input: Number of input neurons\n",
    "        :param n_output: Number of output neurons\n",
    "        :param initializer: Instance of an initializer class (e.g., SimpleInitializer)\n",
    "        :param optimizer: An optimizer class (e.g., SGD)\n",
    "        :param activation: An activation function instance (e.g., Tanh, ReLU, Softmax)\n",
    "        \"\"\"\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation  # Optional activation (e.g., Tanh, Softmax)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W, self.B = self.initializer.initialize(n_input, n_output)\n",
    "        \n",
    "        # Store input and output for backpropagation\n",
    "        self.X = None\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward pass of the fully connected layer.\"\"\"\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.B  # Linear transformation: Z = XW + B\n",
    "        \n",
    "        if self.activation:\n",
    "            # Apply activation function if provided (e.g., Tanh, ReLU, Softmax)\n",
    "            return self.activation.forward(self.Z)\n",
    "        return self.Z  # If no activation is specified (for output layer, usually Softmax)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"Perform the backward pass of the fully connected layer.\"\"\"\n",
    "        # Compute gradients for W, B, and input X\n",
    "        dW = np.dot(self.X.T, dA)  # Gradient of the loss with respect to W\n",
    "        dB = np.sum(dA, axis=0, keepdims=True)  # Gradient of the loss with respect to B\n",
    "        dX = np.dot(dA, self.W.T)  # Gradient of the loss with respect to the input X\n",
    "\n",
    "        # Update weights and biases using the optimizer\n",
    "        self.optimizer.update(self, dW, dB)\n",
    "\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example setup\n",
    "n_features = 784  # Number of input features (e.g., for MNIST dataset)\n",
    "n_nodes1 = 64\n",
    "n_nodes2 = 32\n",
    "n_output = 10  # Number of output classes (e.g., 10 classes for MNIST)\n",
    "\n",
    "# Initialize layers and activation functions\n",
    "initializer = SimpleInitializer(sigma=0.01)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "activation1 = Tanh()\n",
    "activation2 = Tanh()\n",
    "activation3 = Softmax()  # Softmax on the output layer\n",
    "\n",
    "# Create layers\n",
    "FC1 = FC(n_features, n_nodes1, initializer, optimizer, activation1)\n",
    "FC2 = FC(n_nodes1, n_nodes2, initializer, optimizer, activation2)\n",
    "FC3 = FC(n_nodes2, n_output, initializer, optimizer, activation3)\n",
    "\n",
    "# Forward pass with dummy data\n",
    "X_train = np.random.randn(100, n_features)  # Example input data (100 samples)\n",
    "A1 = FC1.forward(X_train)\n",
    "Z1 = activation1.forward(A1)\n",
    "\n",
    "A2 = FC2.forward(Z1)\n",
    "Z2 = activation2.forward(A2)\n",
    "\n",
    "A3 = FC3.forward(Z2)\n",
    "Z3 = activation3.forward(A3)\n",
    "\n",
    "# Backward pass with dummy labels (one-hot encoded)\n",
    "Y_train = np.random.randn(100, n_output)  # Example one-hot encoded labels\n",
    "dA3 = activation3.backward(Z3, Y_train)\n",
    "dZ2 = FC3.backward(dA3)\n",
    "dA2 = activation2.backward(dZ2)\n",
    "dZ1 = FC2.backward(dA2)\n",
    "dA1 = activation1.backward(dZ1)\n",
    "dZ0 = FC1.backward(dA1)  # Final gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Compute the forward pass for the ReLU activation function.\n",
    "        \n",
    "        :param Z: Input matrix (or vector) from the previous layer\n",
    "        :return: Output after applying ReLU\n",
    "        \"\"\"\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)  # ReLU operation\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Compute the backward pass for the ReLU activation function.\n",
    "        \n",
    "        :param dA: Gradient of the loss with respect to the output of this layer\n",
    "        :return: Gradient of the loss with respect to the input to this layer\n",
    "        \"\"\"\n",
    "        dZ = dA * (self.Z > 0)  # Gradient is 1 for positive values and 0 for non-positive values\n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass Output (ReLU applied):\n",
      " [[1 0 3]\n",
      " [0 4 0]]\n",
      "Backward Pass Output (Gradient with respect to input):\n",
      " [[ 0.5  0.  -0.2]\n",
      " [-0.   0.4  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize ReLU activation\n",
    "relu = ReLU()\n",
    "\n",
    "# Example input (Z) from a previous layer\n",
    "Z = np.array([[1, -2, 3], [-1, 4, -5]])\n",
    "\n",
    "# Forward pass (applying ReLU)\n",
    "A = relu.forward(Z)\n",
    "print(\"Forward Pass Output (ReLU applied):\\n\", A)\n",
    "\n",
    "# Assume some gradient from the next layer (dA)\n",
    "dA = np.array([[0.5, 0.3, -0.2], [-0.1, 0.4, 0.6]])\n",
    "\n",
    "# Backward pass (computing gradient)\n",
    "dZ = relu.backward(dA)\n",
    "print(\"Backward Pass Output (Gradient with respect to input):\\n\", dZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Initialize weights using Xavier (Glorot) initialization.\n",
    "        \n",
    "        :param n_in: Number of nodes in the previous layer (input layer)\n",
    "        :param n_out: Number of nodes in the current layer (output layer)\n",
    "        :return: Initialized weights (W) and biases (B)\n",
    "        \"\"\"\n",
    "        # Xavier initialization for weights\n",
    "        sigma = np.sqrt(1 / n_in)\n",
    "        W = np.random.randn(n_in, n_out) * sigma\n",
    "        b = np.zeros((1, n_out))  # Biases are initialized to zero\n",
    "        \n",
    "        return W, b\n",
    "\n",
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Initialize weights using He initialization.\n",
    "        \n",
    "        :param n_in: Number of nodes in the previous layer (input layer)\n",
    "        :param n_out: Number of nodes in the current layer (output layer)\n",
    "        :return: Initialized weights (W) and biases (B)\n",
    "        \"\"\"\n",
    "        # He initialization for weights\n",
    "        sigma = np.sqrt(2 / n_in)\n",
    "        W = np.random.randn(n_in, n_out) * sigma\n",
    "        b = np.zeros((1, n_out))  # Biases are initialized to zero\n",
    "        \n",
    "        return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xavier Weights:\n",
      " [[ 0.0861857  -0.16266965 -0.05053302 ... -0.11015829 -0.21492996\n",
      "  -0.02172654]\n",
      " [-0.00898772 -0.032822    0.04810001 ... -0.16419665  0.03569282\n",
      "  -0.0476632 ]\n",
      " [-0.06455605 -0.06852041 -0.15221713 ...  0.04357356 -0.24574617\n",
      "   0.12336446]\n",
      " ...\n",
      " [ 0.14367959 -0.00755765  0.02849192 ...  0.09332801 -0.10079359\n",
      "   0.06397672]\n",
      " [ 0.10209337 -0.11952592 -0.04900946 ... -0.07109031  0.10700888\n",
      "   0.18193744]\n",
      " [ 0.27755491 -0.01005078 -0.04006266 ... -0.28830225 -0.01350839\n",
      "   0.23071344]]\n",
      "Xavier Biases:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "He Weights:\n",
      " [[-0.02375718  0.15438857 -0.1002378  ...  0.09070818  0.11184704\n",
      "   0.06930497]\n",
      " [-0.19374164 -0.06927836 -0.1940407  ... -0.14169279 -0.00745281\n",
      "   0.12793834]\n",
      " [-0.01553759  0.19700859 -0.04733803 ...  0.16412604 -0.00957297\n",
      "   0.26285529]\n",
      " ...\n",
      " [-0.10906135 -0.32989565  0.05869485 ...  0.04734687 -0.24893115\n",
      "   0.09299013]\n",
      " [ 0.10717501 -0.11926985  0.03354259 ...  0.2220971  -0.09917348\n",
      "   0.13778203]\n",
      " [-0.09675587  0.16222655 -0.04010643 ... -0.00570198  0.18338963\n",
      "   0.2552464 ]]\n",
      "He Biases:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example to initialize weights and biases for a fully connected layer\n",
    "n_in = 64  # Number of nodes in the previous layer\n",
    "n_out = 32  # Number of nodes in the current layer\n",
    "\n",
    "# Xavier Initialization\n",
    "xavier_initializer = XavierInitializer()\n",
    "W_xavier, b_xavier = xavier_initializer.initialize(n_in, n_out)\n",
    "print(\"Xavier Weights:\\n\", W_xavier)\n",
    "print(\"Xavier Biases:\\n\", b_xavier)\n",
    "\n",
    "# He Initialization\n",
    "he_initializer = HeInitializer()\n",
    "W_he, b_he = he_initializer.initialize(n_in, n_out)\n",
    "print(\"He Weights:\\n\", W_he)\n",
    "print(\"He Biases:\\n\", b_he)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        Initialize AdaGrad optimizer.\n",
    "\n",
    "        :param lr: Learning rate (Î±)\n",
    "        :param epsilon: Small constant to avoid division by zero\n",
    "        \"\"\"\n",
    "        self.lr = lr  # Initial learning rate\n",
    "        self.epsilon = epsilon  # Stability constant\n",
    "        self.h = {}  # Dictionary to store sum of squared gradients for each parameter\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update the parameters of a given layer using AdaGrad.\n",
    "\n",
    "        :param layer: Layer object containing weights, biases, and their gradients\n",
    "        \"\"\"\n",
    "        if id(layer) not in self.h:\n",
    "            # Initialize sum of squared gradients for weights and biases\n",
    "            self.h[id(layer)] = {\n",
    "                \"W\": np.zeros_like(layer.W),\n",
    "                \"b\": np.zeros_like(layer.b)\n",
    "            }\n",
    "        \n",
    "        # Accumulate squared gradients\n",
    "        self.h[id(layer)][\"W\"] += layer.dW**2\n",
    "        self.h[id(layer)][\"b\"] += layer.db**2\n",
    "        \n",
    "        # Update weights and biases\n",
    "        layer.W -= self.lr / (np.sqrt(self.h[id(layer)][\"W\"]) + self.epsilon) * layer.dW\n",
    "        layer.b -= self.lr / (np.sqrt(self.h[id(layer)][\"b\"]) + self.epsilon) * layer.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights:\n",
      " [[-0.02254251  0.21585949 -0.20442313 ...  0.12405564 -0.15554432\n",
      "  -0.12719871]\n",
      " [ 0.12488547  0.0889455   0.24226959 ...  0.00848193 -0.0721986\n",
      "   0.0312244 ]\n",
      " [-0.12826705 -0.21338198  0.11751475 ... -0.07816784 -0.09365522\n",
      "  -0.03190996]\n",
      " ...\n",
      " [ 0.08845041  0.00965569  0.16206553 ...  0.06791312  0.03962841\n",
      "  -0.0807056 ]\n",
      " [-0.01634398  0.08789393  0.07498233 ...  0.0900876   0.08573556\n",
      "   0.25233455]\n",
      " [ 0.00835489  0.076099    0.12832031 ...  0.08412204  0.09289088\n",
      "   0.11231759]]\n",
      "Updated Biases:\n",
      " [[-0.01       -0.01        0.01        0.00999992 -0.01        0.01\n",
      "   0.01       -0.01        0.01        0.01       -0.01        0.01\n",
      "   0.01       -0.01       -0.00999999 -0.00999996  0.01       -0.01\n",
      "   0.00999999  0.01        0.01       -0.01        0.01        0.01\n",
      "   0.00999973  0.01       -0.01        0.01        0.01        0.01\n",
      "   0.01       -0.01      ]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming a Layer class exists with attributes: W, b, dW, db\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, initializer):\n",
    "        self.W, self.b = initializer.initialize(input_size, output_size)\n",
    "        self.dW = None  # Placeholder for weight gradients\n",
    "        self.db = None  # Placeholder for bias gradients\n",
    "\n",
    "# Initialize a layer and AdaGrad optimizer\n",
    "n_in, n_out = 64, 32\n",
    "initializer = XavierInitializer()\n",
    "layer = Layer(n_in, n_out, initializer)\n",
    "\n",
    "optimizer = AdaGrad(lr=0.01)\n",
    "\n",
    "# Example: Perform an update (assuming gradients are calculated elsewhere)\n",
    "layer.dW = np.random.randn(n_in, n_out)  # Example gradient\n",
    "layer.db = np.random.randn(1, n_out)    # Example gradient\n",
    "\n",
    "optimizer.update(layer)\n",
    "\n",
    "# Check updated weights and biases\n",
    "print(\"Updated Weights:\\n\", layer.W)\n",
    "print(\"Updated Biases:\\n\", layer.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    def __init__(self, n_features, n_nodes_list, n_output, initializer, optimizer, activations, lr=0.01):\n",
    "        \"\"\"\n",
    "        Initialize a deep neural network classifier.\n",
    "        \n",
    "        :param n_features: Number of input features.\n",
    "        :param n_nodes_list: List containing the number of nodes for each hidden layer.\n",
    "        :param n_output: Number of output classes.\n",
    "        :param initializer: Initializer instance for weights.\n",
    "        :param optimizer: Optimizer instance for gradient descent.\n",
    "        :param activations: List of activation function instances for each layer (including the output layer).\n",
    "        :param lr: Learning rate (used by the optimizer).\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        self.activations = activations\n",
    "\n",
    "        # Build the layers of the neural network\n",
    "        input_dim = n_features\n",
    "        for nodes, activation in zip(n_nodes_list, activations[:-1]):\n",
    "            self.layers.append(Layer(input_dim, nodes, initializer, optimizer))\n",
    "            input_dim = nodes  # Update input dimension for the next layer\n",
    "\n",
    "        # Add the output layer\n",
    "        self.layers.append(Layer(input_dim, n_output, initializer, optimizer))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        :param X: Input data.\n",
    "        :return: Output of the final layer.\n",
    "        \"\"\"\n",
    "        self.inputs = [X]\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            X = layer.forward(X)\n",
    "            X = activation.forward(X)\n",
    "            self.inputs.append(X)  # Save intermediate outputs for backpropagation\n",
    "        return X\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform a backward pass through the network.\n",
    "\n",
    "        :param y_pred: Predicted output (from the forward pass).\n",
    "        :param y_true: True labels (one-hot encoded).\n",
    "        \"\"\"\n",
    "        # Compute the initial gradient using the softmax + cross-entropy derivative\n",
    "        dA = self.activations[-1].backward(y_pred, y_true)\n",
    "\n",
    "        # Backpropagate through each layer\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            dA = self.activations[i].backward(dA)\n",
    "            dA = self.layers[i].backward(dA)\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "\n",
    "        :param X_train: Training data (features).\n",
    "        :param y_train: Training labels (one-hot encoded).\n",
    "        :param epochs: Number of epochs to train for.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X_train)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(y_pred, y_train)\n",
    "\n",
    "            # Calculate and print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                loss = -np.mean(y_train * np.log(y_pred + 1e-7))  # Cross-entropy loss\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for the input data.\n",
    "\n",
    "        :param X: Input data.\n",
    "        :return: Predicted class labels.\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities for the input data.\n",
    "\n",
    "        :param X: Input data.\n",
    "        :return: Predicted class probabilities.\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Initialize a fully connected layer.\n",
    "\n",
    "        :param input_size: Number of input features.\n",
    "        :param output_size: Number of nodes in the layer.\n",
    "        :param initializer: Initializer instance for weights.\n",
    "        :param optimizer: Optimizer instance for updating parameters.\n",
    "        \"\"\"\n",
    "        self.W, self.b = initializer.initialize(input_size, output_size)\n",
    "        self.optimizer = optimizer\n",
    "        self.dW = None  # Placeholder for weight gradients\n",
    "        self.db = None  # Placeholder for bias gradients\n",
    "        self.X = None   # Placeholder for input to the layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the layer.\n",
    "\n",
    "        :param X: Input to the layer.\n",
    "        :return: Output of the layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Perform the backward pass through the layer.\n",
    "\n",
    "        :param dA: Gradient of the loss with respect to the layer's output.\n",
    "        :return: Gradient of the loss with respect to the layer's input.\n",
    "        \"\"\"\n",
    "        self.dW = np.dot(self.X.T, dA) / self.X.shape[0]\n",
    "        self.db = np.sum(dA, axis=0, keepdims=True) / self.X.shape[0]\n",
    "        dX = np.dot(dA, self.W.T)\n",
    "        self.optimizer.update(self)  # Update weights and biases\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.Z = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * (self.Z > 0)\n",
    "\n",
    "class SoftmaxWithCrossEntropy:\n",
    "    def forward(self, X):\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def initialize(self, input_size, output_size):\n",
    "        W = np.random.randn(input_size, output_size) / np.sqrt(input_size)\n",
    "        b = np.zeros((1, output_size))\n",
    "        return W, b\n",
    "\n",
    "class HeInitializer:\n",
    "    def initialize(self, input_size, output_size):\n",
    "        W = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "        b = np.zeros((1, output_size))\n",
    "        return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SoftmaxWithCrossEntropy.backward() missing 1 required positional argument: 'y_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(n_output)[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(n_output, \u001b[38;5;241m100\u001b[39m)]  \u001b[38;5;66;03m# One-hot encoded labels\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     30\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "Cell \u001b[1;32mIn[27], line 71\u001b[0m, in \u001b[0;36mScratchDeepNeuralNetworkClassifier.fit\u001b[1;34m(self, X_train, y_train, epochs)\u001b[0m\n\u001b[0;32m     68\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_train)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate and print loss every 100 epochs\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[27], line 55\u001b[0m, in \u001b[0;36mScratchDeepNeuralNetworkClassifier.backward\u001b[1;34m(self, y_pred, y_true)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Backpropagate through each layer\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers))):\n\u001b[1;32m---> 55\u001b[0m     dA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     dA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mbackward(dA)\n",
      "\u001b[1;31mTypeError\u001b[0m: SoftmaxWithCrossEntropy.backward() missing 1 required positional argument: 'y_true'"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "n_features = 10\n",
    "n_nodes_list = [64, 32]\n",
    "n_output = 3\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize components\n",
    "initializer = HeInitializer()\n",
    "optimizer = SGD(lr=0.01)\n",
    "activations = [ReLU(), ReLU(), SoftmaxWithCrossEntropy()]\n",
    "\n",
    "# Create the model\n",
    "model = ScratchDeepNeuralNetworkClassifier(\n",
    "    n_features=n_features, \n",
    "    n_nodes_list=n_nodes_list, \n",
    "    n_output=n_output, \n",
    "    initializer=initializer, \n",
    "    optimizer=optimizer, \n",
    "    activations=activations\n",
    ")\n",
    "\n",
    "# Example data\n",
    "X_train = np.random.randn(100, n_features)\n",
    "Y_train = np.eye(n_output)[np.random.choice(n_output, 100)]  # One-hot encoded labels\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_train)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Perform a backward pass through the network.\n",
    "\n",
    "    :param y_pred: Predicted output (from the forward pass).\n",
    "    :param y_true: True labels (one-hot encoded).\n",
    "    \"\"\"\n",
    "    # Compute the initial gradient for the last layer (Softmax + Cross-Entropy)\n",
    "    dA = self.activations[-1].backward(y_pred, y_true)\n",
    "\n",
    "    # Backpropagate through each hidden layer\n",
    "    for i in reversed(range(len(self.layers) - 1)):\n",
    "        dA = self.activations[i].backward(dA)\n",
    "        dA = self.layers[i].backward(dA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithCrossEntropy:\n",
    "    def forward(self, X):\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute gradient for Softmax + Cross-Entropy.\n",
    "        \"\"\"\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    def __init__(self, n_features, n_nodes_list, n_output, activations, initializers, optimizers, lr=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the deep neural network classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - n_features: int, number of input features\n",
    "        - n_nodes_list: list, number of nodes in each hidden layer\n",
    "        - n_output: int, number of output classes\n",
    "        - activations: list, activation function instances (one per layer)\n",
    "        - initializers: list, initializer instances (one per layer)\n",
    "        - optimizers: list, optimizer instances (one per layer)\n",
    "        - lr: float, learning rate\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        self.activations = activations\n",
    "\n",
    "        # Create layers (hidden + output)\n",
    "        n_layers = len(n_nodes_list)\n",
    "        layer_sizes = [n_features] + n_nodes_list + [n_output]\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(\n",
    "                FullyConnectedLayer(\n",
    "                    n_input=layer_sizes[i],\n",
    "                    n_output=layer_sizes[i + 1],\n",
    "                    initializer=initializers[i],\n",
    "                    optimizer=optimizers[i]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward propagation.\"\"\"\n",
    "        A = X\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            Z = layer.forward(A)\n",
    "            A = activation.forward(Z)\n",
    "        return A\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Perform backward propagation.\"\"\"\n",
    "        # Start with the output layer gradient (Softmax + Cross-Entropy)\n",
    "        dA = self.activations[-1].backward(y_pred, y_true)\n",
    "\n",
    "        # Backpropagate through each layer\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            dA = self.layers[i].backward(dA)\n",
    "            dA = self.activations[i - 1].backward(dA) if i > 0 else dA\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=1000):\n",
    "        \"\"\"Train the model using the provided data.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X_train)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(y_pred, y_train)\n",
    "\n",
    "            # Calculate and print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.calculate_loss(y_train, y_pred)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for the input data.\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate cross-entropy loss.\"\"\"\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-7), axis=1))\n",
    "\n",
    "\n",
    "# Supporting classes\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, n_input, n_output, initializer, optimizer):\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W = initializer.initialize_weights(n_input, n_output)\n",
    "        self.B = initializer.initialize_biases(n_output)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.B\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dW = np.dot(self.X.T, dZ) / self.X.shape[0]\n",
    "        dB = np.sum(dZ, axis=0, keepdims=True) / self.X.shape[0]\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W = self.optimizer.update(self.W, dW)\n",
    "        self.B = self.optimizer.update(self.B, dB)\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class SoftmaxWithCrossEntropy:\n",
    "    def forward(self, Z):\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        self.A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * (self.Z > 0)\n",
    "\n",
    "\n",
    "class XavierInitializer:\n",
    "    def initialize_weights(self, n_input, n_output):\n",
    "        return np.random.randn(n_input, n_output) * np.sqrt(1 / n_input)\n",
    "\n",
    "    def initialize_biases(self, n_output):\n",
    "        return np.zeros((1, n_output))\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, param, grad):\n",
    "        if self.h is None:\n",
    "            self.h = np.zeros_like(grad)\n",
    "        self.h += grad ** 2\n",
    "        adjusted_lr = self.lr / (np.sqrt(self.h) + 1e-7)\n",
    "        return param - adjusted_lr * grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_val, y_val, n_features, n_output, layer_configs, initializer, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate a deep neural network.\n",
    "\n",
    "    Parameters:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels (one-hot encoded)\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels (one-hot encoded)\n",
    "        n_features: Number of features in the input\n",
    "        n_output: Number of output classes\n",
    "        layer_configs: List of tuples [(n_nodes, activation), ...]\n",
    "        initializer: Weight initializer instance\n",
    "        optimizer: Optimizer instance\n",
    "        epochs: Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        accuracy: Validation accuracy\n",
    "    \"\"\"\n",
    "    # Create the neural network\n",
    "    model = ScratchDeepNeuralNetworkClassifier(\n",
    "        n_features=n_features,\n",
    "        n_nodes_list=[config[0] for config in layer_configs],\n",
    "        n_output=n_output,\n",
    "        activations=[config[1] for config in layer_configs],\n",
    "        initializer=initializer,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs)\n",
    "\n",
    "    # Predict on validation set\n",
    "    predictions = model.predict(X_val)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mnist.target.astype(int)  # Replace np.int with the built-in int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mnist.target.astype(np.int32)  # or np.int64, depending on the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_val, y_val, n_features, n_output, layer_configs, initializer, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate a deep neural network.\n",
    "\n",
    "    Parameters:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels (one-hot encoded)\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels (one-hot encoded)\n",
    "        n_features: Number of features in the input\n",
    "        n_output: Number of output classes\n",
    "        layer_configs: List of tuples [(n_nodes, activation), ...]\n",
    "        initializer: Weight initializer instance\n",
    "        optimizer: Optimizer instance\n",
    "        epochs: Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        accuracy: Validation accuracy\n",
    "    \"\"\"\n",
    "    # Create the neural network\n",
    "    model = ScratchDeepNeuralNetworkClassifier(\n",
    "        n_features=n_features,\n",
    "        n_nodes_list=[config[0] for config in layer_configs],\n",
    "        n_output=n_output,\n",
    "        activations=[config[1] for config in layer_configs],\n",
    "        initializer=initializer,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs)\n",
    "\n",
    "    # Predict on validation set\n",
    "    predictions = model.predict(X_val)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracies\n",
    "plt.bar(results.keys(), results.values(), color=['blue', 'green', 'orange'])\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Network Configuration\")\n",
    "plt.title(\"Comparison of Network Architectures\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
