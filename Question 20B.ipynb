{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Constructor for the FullyConnectedLayer class.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size (int): The size of the input to the layer.\n",
    "        - output_size (int): The size of the output from the layer.\n",
    "        - initializer: Instance of a weight initialization class.\n",
    "        - optimizer: Instance of an optimization method class.\n",
    "        \"\"\"\n",
    "        # Initialize weights and biases\n",
    "        self.W = initializer.initialize((input_size, output_size))  # Weight matrix\n",
    "        self.B = initializer.initialize((1, output_size))          # Biases\n",
    "        \n",
    "        # Save the optimizer instance\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Variables to store inputs and gradients for backpropagation\n",
    "        self.X = None  # Input data\n",
    "        self.dW = None # Gradient for weights\n",
    "        self.dB = None # Gradient for biases\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input to the layer, of shape (batch_size, input_size).\n",
    "        \n",
    "        Returns:\n",
    "        - Output of the layer, of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for backward pass\n",
    "        return np.dot(X, self.W) + self.B  # Linear transformation\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Perform the backward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of the loss with respect to the output, \n",
    "          of shape (batch_size, output_size).\n",
    "        \n",
    "        Returns:\n",
    "        - dX (numpy.ndarray): Gradient of the loss with respect to the input, \n",
    "          of shape (batch_size, input_size).\n",
    "        \"\"\"\n",
    "        # Compute gradients for weights and biases\n",
    "        self.dW = np.dot(self.X.T, dY)  # Gradient of weights\n",
    "        self.dB = np.sum(dY, axis=0, keepdims=True)  # Gradient of biases\n",
    "\n",
    "        # Compute gradient for the input (for propagation to earlier layers)\n",
    "        dX = np.dot(dY, self.W.T)\n",
    "        \n",
    "        # Update weights and biases using the optimizer\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    def initialize(self, shape):\n",
    "        # Example: Xavier/Glorot initialization\n",
    "        return np.random.randn(*shape) * np.sqrt(2 / sum(shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, layer):\n",
    "        # Update weights and biases\n",
    "        layer.W -= self.learning_rate * layer.dW\n",
    "        layer.B -= self.learning_rate * layer.dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output: [[-0.42665373 -0.39870235 -0.67526294]\n",
      " [-0.53398361  0.05923895 -0.73042137]\n",
      " [-0.01678972 -0.37778002 -1.17405806]\n",
      " [-0.31082859  0.24465106 -0.24983533]\n",
      " [-0.2640523  -0.17558532 -0.89977182]]\n",
      "Gradient Input: [[ 0.02208399  0.39782356 -0.30132513 -0.26939947]\n",
      " [-0.25548192  0.06477422 -0.12605292 -0.85485563]\n",
      " [-0.02180358  0.40603272 -0.41200395 -0.48534439]\n",
      " [-0.21842685 -0.09020194  0.10674821 -0.5271222 ]\n",
      " [ 0.02846066  0.51665086 -0.42660745 -0.37816668]]\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the initializer and optimizer\n",
    "initializer = Initializer()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "\n",
    "# Create a fully connected layer\n",
    "fc_layer = FullyConnectedLayer(input_size=4, output_size=3, initializer=initializer, optimizer=optimizer)\n",
    "\n",
    "# Perform forward pass\n",
    "X = np.random.rand(5, 4)  # Example input with batch_size=5, input_size=4\n",
    "output = fc_layer.forward(X)\n",
    "print(\"Forward Output:\", output)\n",
    "\n",
    "# Perform backward pass\n",
    "dY = np.random.rand(5, 3)  # Example gradient from the next layer\n",
    "grad_input = fc_layer.backward(dY)\n",
    "print(\"Gradient Input:\", grad_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma=0.01):\n",
    "        \"\"\"\n",
    "        Constructor for the SimpleInitializer class.\n",
    "        \n",
    "        Parameters:\n",
    "        - sigma (float): Standard deviation for initializing weights and biases.\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def initialize(self, shape):\n",
    "        \"\"\"\n",
    "        Initialize weights or biases with a normal distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        - shape (tuple): Shape of the array to be initialized.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Initialized array.\n",
    "        \"\"\"\n",
    "        return np.random.randn(*shape) * self.sigma\n",
    "\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Constructor for the FullyConnectedLayer class.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size (int): The size of the input to the layer.\n",
    "        - output_size (int): The size of the output from the layer.\n",
    "        - initializer: Instance of the initialization class (e.g., SimpleInitializer).\n",
    "        - optimizer: Instance of the optimization method class.\n",
    "        \"\"\"\n",
    "        # Initialize weights and biases using the provided initializer\n",
    "        self.W = initializer.initialize((input_size, output_size))  # Weight matrix\n",
    "        self.B = initializer.initialize((1, output_size))          # Biases\n",
    "        \n",
    "        # Save the optimizer instance\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Variables to store inputs and gradients for backpropagation\n",
    "        self.X = None  # Input data\n",
    "        self.dW = None # Gradient for weights\n",
    "        self.dB = None # Gradient for biases\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input to the layer, of shape (batch_size, input_size).\n",
    "        \n",
    "        Returns:\n",
    "        - Output of the layer, of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for backward pass\n",
    "        return np.dot(X, self.W) + self.B  # Linear transformation\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Perform the backward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of the loss with respect to the output, \n",
    "          of shape (batch_size, output_size).\n",
    "        \n",
    "        Returns:\n",
    "        - dX (numpy.ndarray): Gradient of the loss with respect to the input, \n",
    "          of shape (batch_size, input_size).\n",
    "        \"\"\"\n",
    "        # Compute gradients for weights and biases\n",
    "        self.dW = np.dot(self.X.T, dY)  # Gradient of weights\n",
    "        self.dB = np.sum(dY, axis=0, keepdims=True)  # Gradient of biases\n",
    "\n",
    "        # Compute gradient for the input (for propagation to earlier layers)\n",
    "        dX = np.dot(dY, self.W.T)\n",
    "        \n",
    "        # Update weights and biases using the optimizer\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output:\n",
      " [[ 0.05412712 -0.00554765  0.02775918]\n",
      " [ 0.03946052 -0.07849734 -0.04296552]\n",
      " [ 0.10248481 -0.05831439 -0.07313973]\n",
      " [ 0.09435305  0.01470969  0.02149186]\n",
      " [ 0.0672845  -0.05236816 -0.04318388]]\n",
      "Gradient Input:\n",
      " [[-0.02995465  0.08489923  0.03508099 -0.13184909]\n",
      " [-0.01769098  0.0762486   0.0267606  -0.1236728 ]\n",
      " [ 0.03560946  0.10126048  0.11369913 -0.02493133]\n",
      " [-0.01566642  0.03166856  0.0035139  -0.06555549]\n",
      " [-0.0157688   0.05562916  0.00934809 -0.10715724]]\n"
     ]
    }
   ],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, layer):\n",
    "        # Update weights and biases\n",
    "        layer.W -= self.learning_rate * layer.dW\n",
    "        layer.B -= self.learning_rate * layer.dB\n",
    "\n",
    "\n",
    "# Create instances of the initializer and optimizer\n",
    "initializer = SimpleInitializer(sigma=0.1)\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "\n",
    "# Create a fully connected layer\n",
    "fc_layer = FullyConnectedLayer(input_size=4, output_size=3, initializer=initializer, optimizer=optimizer)\n",
    "\n",
    "# Perform forward pass\n",
    "X = np.random.rand(5, 4)  # Example input with batch_size=5, input_size=4\n",
    "output = fc_layer.forward(X)\n",
    "print(\"Forward Output:\\n\", output)\n",
    "\n",
    "# Perform backward pass\n",
    "dY = np.random.rand(5, 3)  # Example gradient from the next layer\n",
    "grad_input = fc_layer.backward(dY)\n",
    "print(\"Gradient Input:\\n\", grad_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Constructor for the SGD optimizer class.\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate (float): Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of the given layer using SGD.\n",
    "        \n",
    "        Parameters:\n",
    "        - layer (FullyConnectedLayer): The layer whose parameters will be updated.\n",
    "        \"\"\"\n",
    "        # Update weights and biases using the computed gradients\n",
    "        layer.W -= self.learning_rate * layer.dW\n",
    "        layer.B -= self.learning_rate * layer.dB\n",
    "\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Constructor for the FullyConnectedLayer class.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size (int): The size of the input to the layer.\n",
    "        - output_size (int): The size of the output from the layer.\n",
    "        - initializer: Instance of the initialization class (e.g., SimpleInitializer).\n",
    "        - optimizer: Instance of the optimization method class (e.g., SGD).\n",
    "        \"\"\"\n",
    "        # Initialize weights and biases using the provided initializer\n",
    "        self.W = initializer.initialize((input_size, output_size))  # Weight matrix\n",
    "        self.B = initializer.initialize((1, output_size))          # Biases\n",
    "        \n",
    "        # Save the optimizer instance\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Variables to store inputs and gradients for backpropagation\n",
    "        self.X = None  # Input data\n",
    "        self.dW = None # Gradient for weights\n",
    "        self.dB = None # Gradient for biases\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input to the layer, of shape (batch_size, input_size).\n",
    "        \n",
    "        Returns:\n",
    "        - Output of the layer, of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for backward pass\n",
    "        return np.dot(X, self.W) + self.B  # Linear transformation\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Perform the backward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of the loss with respect to the output, \n",
    "          of shape (batch_size, output_size).\n",
    "        \n",
    "        Returns:\n",
    "        - dX (numpy.ndarray): Gradient of the loss with respect to the input, \n",
    "          of shape (batch_size, input_size).\n",
    "        \"\"\"\n",
    "        # Compute gradients for weights and biases\n",
    "        self.dW = np.dot(self.X.T, dY)  # Gradient of weights\n",
    "        self.dB = np.sum(dY, axis=0, keepdims=True)  # Gradient of biases\n",
    "\n",
    "        # Compute gradient for the input (for propagation to earlier layers)\n",
    "        dX = np.dot(dY, self.W.T)\n",
    "        \n",
    "        # Update weights and biases using the optimizer\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output:\n",
      " [[ 0.08599222  0.18257907 -0.0156909 ]\n",
      " [ 0.08510483  0.07190332  0.01612486]\n",
      " [-0.02717506 -0.02700944  0.05409878]\n",
      " [-0.00112033  0.02751845  0.05442566]\n",
      " [ 0.00376513 -0.00527917  0.13977493]]\n",
      "Gradient Input:\n",
      " [[ 0.03916811 -0.02896058 -0.10501562 -0.05243317]\n",
      " [ 0.06770587 -0.00338589 -0.12000314 -0.13286546]\n",
      " [ 0.10997139  0.13648482 -0.09795272 -0.28434363]\n",
      " [ 0.10821447  0.05064348 -0.08598406 -0.28428584]\n",
      " [ 0.12422543  0.03028878 -0.13200677 -0.30341606]]\n"
     ]
    }
   ],
   "source": [
    "# Define an initializer\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def initialize(self, shape):\n",
    "        return np.random.randn(*shape) * self.sigma\n",
    "\n",
    "\n",
    "# Create instances of the initializer and optimizer\n",
    "initializer = SimpleInitializer(sigma=0.1)\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "\n",
    "# Create a fully connected layer\n",
    "fc_layer = FullyConnectedLayer(input_size=4, output_size=3, initializer=initializer, optimizer=optimizer)\n",
    "\n",
    "# Perform forward pass\n",
    "X = np.random.rand(5, 4)  # Example input with batch_size=5, input_size=4\n",
    "output = fc_layer.forward(X)\n",
    "print(\"Forward Output:\\n\", output)\n",
    "\n",
    "# Perform backward pass\n",
    "dY = np.random.rand(5, 3)  # Example gradient from the next layer\n",
    "grad_input = fc_layer.backward(dY)\n",
    "print(\"Gradient Input:\\n\", grad_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.mW = None\n",
    "        self.vW = None\n",
    "        self.mB = None\n",
    "        self.vB = None\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        if self.mW is None:\n",
    "            # Initialize moment estimates to zero\n",
    "            self.mW = np.zeros_like(layer.W)\n",
    "            self.vW = np.zeros_like(layer.W)\n",
    "            self.mB = np.zeros_like(layer.B)\n",
    "            self.vB = np.zeros_like(layer.B)\n",
    "        \n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update biased first and second moment estimates\n",
    "        self.mW = self.beta1 * self.mW + (1 - self.beta1) * layer.dW\n",
    "        self.vW = self.beta2 * self.vW + (1 - self.beta2) * (layer.dW ** 2)\n",
    "        self.mB = self.beta1 * self.mB + (1 - self.beta1) * layer.dB\n",
    "        self.vB = self.beta2 * self.vB + (1 - self.beta2) * (layer.dB ** 2)\n",
    "\n",
    "        # Correct bias in moments\n",
    "        mW_hat = self.mW / (1 - self.beta1 ** self.t)\n",
    "        vW_hat = self.vW / (1 - self.beta2 ** self.t)\n",
    "        mB_hat = self.mB / (1 - self.beta1 ** self.t)\n",
    "        vB_hat = self.vB / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # Update weights and biases\n",
    "        layer.W -= self.learning_rate * mW_hat / (np.sqrt(vW_hat) + self.epsilon)\n",
    "        layer.B -= self.learning_rate * mB_hat / (np.sqrt(vB_hat) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for ReLU activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Output after applying ReLU.\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for backward pass\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Backward pass for ReLU activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of loss with respect to the output of ReLU.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Gradient of loss with respect to the input of ReLU.\n",
    "        \"\"\"\n",
    "        return dY * (self.X > 0)  # Derivative of ReLU\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for Sigmoid activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Output after applying Sigmoid.\n",
    "        \"\"\"\n",
    "        self.output = 1 / (1 + np.exp(-X))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Backward pass for Sigmoid activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of loss with respect to the output of Sigmoid.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Gradient of loss with respect to the input of Sigmoid.\n",
    "        \"\"\"\n",
    "        return dY * self.output * (1 - self.output)  # Derivative of Sigmoid\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for Tanh activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Output after applying Tanh.\n",
    "        \"\"\"\n",
    "        self.output = np.tanh(X)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Backward pass for Tanh activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of loss with respect to the output of Tanh.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Gradient of loss with respect to the input of Tanh.\n",
    "        \"\"\"\n",
    "        return dY * (1 - self.output ** 2)  # Derivative of Tanh\n",
    "\n",
    "\n",
    "class SoftmaxWithCrossEntropy:\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"\n",
    "        Forward pass for Softmax activation combined with cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input array (logits), shape (batch_size, num_classes).\n",
    "        - y (numpy.ndarray): One-hot encoded labels, shape (batch_size, num_classes).\n",
    "        \n",
    "        Returns:\n",
    "        - float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # Compute softmax\n",
    "        X_exp = np.exp(X - np.max(X, axis=1, keepdims=True))  # Stability adjustment\n",
    "        self.probs = X_exp / np.sum(X_exp, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        self.y = y\n",
    "        batch_size = X.shape[0]\n",
    "        self.loss = -np.sum(y * np.log(self.probs + 1e-12)) / batch_size  # Add epsilon to avoid log(0)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward pass for Softmax activation combined with cross-entropy loss.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Gradient of loss with respect to the input logits.\n",
    "        \"\"\"\n",
    "        batch_size = self.y.shape[0]\n",
    "        return (self.probs - self.y) / batch_size  # Gradient of loss w.r.t. logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.31853976964780395\n",
      "Gradient w.r.t. Input:\n",
      " [[-0.17049943  0.12121649  0.04928295]\n",
      " [ 0.05430187 -0.09876047  0.04445861]]\n"
     ]
    }
   ],
   "source": [
    "# Example input data and labels\n",
    "X = np.array([[2.0, 1.0, 0.1],\n",
    "              [0.5, 2.5, 0.3]])\n",
    "y = np.array([[1, 0, 0],\n",
    "              [0, 1, 0]])  # One-hot encoded labels\n",
    "\n",
    "# Softmax with Cross-Entropy\n",
    "softmax_ce = SoftmaxWithCrossEntropy()\n",
    "\n",
    "# Forward pass\n",
    "loss = softmax_ce.forward(X, y)\n",
    "print(\"Cross-Entropy Loss:\", loss)\n",
    "\n",
    "# Backward pass\n",
    "dX = softmax_ce.backward()\n",
    "print(\"Gradient w.r.t. Input:\\n\", dX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fully connected layer with ReLU activation\n",
    "fc_layer = FullyConnectedLayer(input_size=4, output_size=3, initializer=initializer, optimizer=optimizer)\n",
    "relu = ReLU()\n",
    "\n",
    "# Forward pass\n",
    "X = np.random.rand(5, 4)\n",
    "fc_output = fc_layer.forward(X)\n",
    "relu_output = relu.forward(fc_output)\n",
    "\n",
    "# Backward pass\n",
    "dY = np.random.rand(5, 3)  # Example gradient\n",
    "relu_grad = relu.backward(dY)\n",
    "fc_grad = fc_layer.backward(relu_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for the ReLU activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input array (features).\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Output after applying ReLU.\n",
    "        \"\"\"\n",
    "        self.X = X  # Store input for use in backward pass\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Backward pass for the ReLU activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of the loss with respect to the output of ReLU.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: Gradient of the loss with respect to the input of ReLU.\n",
    "        \"\"\"\n",
    "        dX = dY * (self.X > 0)  # Gradient is 1 if input > 0, otherwise 0\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output:\n",
      " [[0. 2. 0.]\n",
      " [3. 0. 0.]]\n",
      "Backward Output (Gradient):\n",
      " [[0.  2.  0. ]\n",
      " [0.1 0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Example input array\n",
    "X = np.array([[-1.0, 2.0, -0.5], \n",
    "              [3.0, -0.1, 0.0]])\n",
    "\n",
    "# Instantiate the ReLU class\n",
    "relu = ReLU()\n",
    "\n",
    "# Forward pass\n",
    "forward_output = relu.forward(X)\n",
    "print(\"Forward Output:\\n\", forward_output)\n",
    "\n",
    "# Example gradient from the next layer\n",
    "dY = np.array([[1.0, 2.0, 3.0], \n",
    "               [0.1, 0.2, 0.3]])\n",
    "\n",
    "# Backward pass\n",
    "backward_output = relu.backward(dY)\n",
    "print(\"Backward Output (Gradient):\\n\", backward_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pipeline with FullyConnectedLayer and ReLU\n",
    "fc_layer = FullyConnectedLayer(input_size=4, output_size=3, initializer=initializer, optimizer=optimizer)\n",
    "relu = ReLU()\n",
    "\n",
    "# Forward pass\n",
    "X = np.random.rand(5, 4)  # Example input\n",
    "fc_output = fc_layer.forward(X)\n",
    "relu_output = relu.forward(fc_output)\n",
    "\n",
    "# Backward pass\n",
    "dY = np.random.rand(5, 3)  # Example gradient\n",
    "relu_grad = relu.backward(dY)\n",
    "fc_grad = fc_layer.backward(relu_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def initialize(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases using Xavier initialization.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size (int): Number of nodes in the previous layer.\n",
    "        - output_size (int): Number of nodes in the current layer.\n",
    "        \n",
    "        Returns:\n",
    "        - W (numpy.ndarray): Weight matrix initialized with Xavier method.\n",
    "        - B (numpy.ndarray): Bias vector initialized with zeros.\n",
    "        \"\"\"\n",
    "        sigma = 1.0 / np.sqrt(input_size)\n",
    "        W = np.random.normal(0, sigma, (input_size, output_size))\n",
    "        B = np.zeros(output_size)\n",
    "        return W, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def initialize(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases using He initialization.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size (int): Number of nodes in the previous layer.\n",
    "        - output_size (int): Number of nodes in the current layer.\n",
    "        \n",
    "        Returns:\n",
    "        - W (numpy.ndarray): Weight matrix initialized with He method.\n",
    "        - B (numpy.ndarray): Bias vector initialized with zeros.\n",
    "        \"\"\"\n",
    "        sigma = np.sqrt(2.0 / input_size)\n",
    "        W = np.random.normal(0, sigma, (input_size, output_size))\n",
    "        B = np.zeros(output_size)\n",
    "        return W, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xavier Weights Shape: (128, 64)\n",
      "Xavier Bias Shape: (64,)\n",
      "He Weights Shape: (128, 64)\n",
      "He Bias Shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# Example: Xavier Initializer\n",
    "xavier_init = XavierInitializer()\n",
    "W_xavier, B_xavier = xavier_init.initialize(input_size=128, output_size=64)\n",
    "print(\"Xavier Weights Shape:\", W_xavier.shape)\n",
    "print(\"Xavier Bias Shape:\", B_xavier.shape)\n",
    "\n",
    "# Example: He Initializer\n",
    "he_init = HeInitializer()\n",
    "W_he, B_he = he_init.initialize(input_size=128, output_size=64)\n",
    "print(\"He Weights Shape:\", W_he.shape)\n",
    "print(\"He Bias Shape:\", B_he.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, initializer, optimizer):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Initialize weights and biases using the initializer\n",
    "        self.W, self.B = self.initializer.initialize(input_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.dot(X, self.W) + self.B\n",
    "\n",
    "    def backward(self, dY):\n",
    "        dW = np.dot(self.X.T, dY)\n",
    "        dB = np.sum(dY, axis=0)\n",
    "        dX = np.dot(dY, self.W.T)\n",
    "\n",
    "        # Update weights and biases using the optimizer\n",
    "        self.W, self.B = self.optimizer.update(self, dW, dB)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Initializes the AdaGrad optimizer.\n",
    "\n",
    "        Parameters:\n",
    "        - learning_rate (float): Initial learning rate (alpha).\n",
    "        - epsilon (float): Small value to avoid division by zero.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.H_W = None  # Accumulated squared gradients for weights\n",
    "        self.H_B = None  # Accumulated squared gradients for biases\n",
    "\n",
    "    def update(self, layer, dW, dB):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of a layer using AdaGrad optimization.\n",
    "\n",
    "        Parameters:\n",
    "        - layer: The layer instance containing weights (W) and biases (B).\n",
    "        - dW (numpy.ndarray): Gradient of loss with respect to weights.\n",
    "        - dB (numpy.ndarray): Gradient of loss with respect to biases.\n",
    "\n",
    "        Returns:\n",
    "        - Updated weights (W) and biases (B).\n",
    "        \"\"\"\n",
    "        # Initialize H_W and H_B if not already initialized\n",
    "        if self.H_W is None:\n",
    "            self.H_W = np.zeros_like(layer.W)\n",
    "        if self.H_B is None:\n",
    "            self.H_B = np.zeros_like(layer.B)\n",
    "\n",
    "        # Update accumulated squared gradients\n",
    "        self.H_W += dW**2\n",
    "        self.H_B += dB**2\n",
    "\n",
    "        # Update weights and biases\n",
    "        layer.W -= self.learning_rate * dW / (np.sqrt(self.H_W) + self.epsilon)\n",
    "        layer.B -= self.learning_rate * dB / (np.sqrt(self.H_B) + self.epsilon)\n",
    "\n",
    "        return layer.W, layer.B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights Shape: (128, 64)\n",
      "Updated Biases Shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "# Example Layer with Dummy Weights and Biases\n",
    "class DummyLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.rand(input_size, output_size)\n",
    "        self.B = np.random.rand(output_size)\n",
    "\n",
    "# Create a Dummy Layer\n",
    "layer = DummyLayer(input_size=128, output_size=64)\n",
    "\n",
    "# Gradients (Example)\n",
    "dW = np.random.rand(128, 64)\n",
    "dB = np.random.rand(64)\n",
    "\n",
    "# Instantiate AdaGrad Optimizer\n",
    "adagrad = AdaGrad(learning_rate=0.01)\n",
    "\n",
    "# Update Weights and Biases\n",
    "updated_W, updated_B = adagrad.update(layer, dW, dB)\n",
    "\n",
    "# Print Updated Parameters\n",
    "print(\"Updated Weights Shape:\", updated_W.shape)\n",
    "print(\"Updated Biases Shape:\", updated_B.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, initializer, optimizer):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W, self.B = self.initializer.initialize(input_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.dot(X, self.W) + self.B\n",
    "\n",
    "    def backward(self, dY):\n",
    "        dW = np.dot(self.X.T, dY)\n",
    "        dB = np.sum(dY, axis=0)\n",
    "        dX = np.dot(dY, self.W.T)\n",
    "\n",
    "        # Update weights and biases using the optimizer\n",
    "        self.W, self.B = self.optimizer.update(self, dW, dB)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    def __init__(self, epochs=10, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Initializes the ScratchDeepNeuralNetworkClassifier.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs (int): Number of training epochs.\n",
    "        - batch_size (int): Size of each training batch.\n",
    "        - verbose (bool): If True, prints training progress.\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.layers = []  # List to store layers of the network\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Adds a layer to the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - layer: Layer instance (e.g., FullyConnectedLayer, ActivationFunction).\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - Output of the last layer (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Performs a backward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - y (numpy.ndarray): True labels for the output.\n",
    "        \"\"\"\n",
    "        grad = y\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Trains the neural network using the provided training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train (numpy.ndarray): Training feature data.\n",
    "        - y_train (numpy.ndarray): Training label data.\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        for epoch in range(self.epochs):\n",
    "            perm = np.random.permutation(n_samples)  # Shuffle the data\n",
    "            X_train = X_train[perm]\n",
    "            y_train = y_train[perm]\n",
    "            \n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                # Create batches\n",
    "                X_batch = X_train[i:i + self.batch_size]\n",
    "                y_batch = y_train[i:i + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # Compute gradient of loss\n",
    "                grad = self.layers[-1].compute_gradient(y_batch)  # Softmax + CrossEntropy layer\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(grad)\n",
    "\n",
    "            # Verbose: Print progress\n",
    "            if self.verbose:\n",
    "                loss = self._compute_loss(X_train, y_train)\n",
    "                acc = self.score(X_train, y_train)\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts labels for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - Predicted labels (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Evaluates the accuracy of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data.\n",
    "        - y (numpy.ndarray): True labels.\n",
    "\n",
    "        Returns:\n",
    "        - Accuracy (float).\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        return np.mean(y_pred == y_true)\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the loss for the given data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data.\n",
    "        - y (numpy.ndarray): True labels.\n",
    "\n",
    "        Returns:\n",
    "        - Loss value (float).\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        loss = -np.sum(y * np.log(y_pred + 1e-8)) / y.shape[0]\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def initialize(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes weights using He initialization.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size (int): Number of input nodes.\n",
    "        - output_size (int): Number of output nodes.\n",
    "\n",
    "        Returns:\n",
    "        - weights (numpy.ndarray): Initialized weights.\n",
    "        - biases (numpy.ndarray): Initialized biases (set to zero).\n",
    "        \"\"\"\n",
    "        stddev = np.sqrt(2.0 / input_size)\n",
    "        weights = np.random.randn(input_size, output_size) * stddev\n",
    "        biases = np.zeros(output_size)\n",
    "        return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        A fully connected (dense) layer.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size (int): Number of input nodes.\n",
    "        - output_size (int): Number of output nodes.\n",
    "        - initializer: Instance of an initializer class.\n",
    "        - optimizer: Instance of an optimizer class.\n",
    "        \"\"\"\n",
    "        self.weights, self.biases = initializer.initialize(input_size, output_size)\n",
    "        self.optimizer = optimizer\n",
    "        self.input = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - Output of the layer (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        return np.dot(X, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Performs the backward pass.\n",
    "\n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of the loss with respect to the output.\n",
    "\n",
    "        Returns:\n",
    "        - Gradient of the loss with respect to the input (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        self.dW = np.dot(self.input.T, dY)\n",
    "        self.db = np.sum(dY, axis=0)\n",
    "        dX = np.dot(dY, self.weights.T)\n",
    "        self.optimizer.update(self)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through ReLU activation.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - Activated output (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        Backward pass through ReLU activation.\n",
    "\n",
    "        Parameters:\n",
    "        - dY (numpy.ndarray): Gradient of the loss with respect to the output.\n",
    "\n",
    "        Returns:\n",
    "        - Gradient of the loss with respect to the input (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        dX = dY * (self.input > 0)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through softmax with cross-entropy.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - Probabilities after softmax (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))  # Stability trick\n",
    "        self.y_pred = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "        return self.y_pred\n",
    "\n",
    "    def compute_gradient(self, y_true):\n",
    "        \"\"\"\n",
    "        Compute the gradient for cross-entropy loss.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true (numpy.ndarray): True one-hot labels.\n",
    "\n",
    "        Returns:\n",
    "        - Gradient of the loss with respect to the input (numpy.ndarray).\n",
    "        \"\"\"\n",
    "        self.y_true = y_true\n",
    "        return (self.y_pred - y_true) / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the AdaGrad optimizer.\n",
    "\n",
    "        Parameters:\n",
    "        - learning_rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.H = {}\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updates weights and biases using AdaGrad.\n",
    "\n",
    "        Parameters:\n",
    "        - layer: Instance of a fully connected layer.\n",
    "        \"\"\"\n",
    "        if layer not in self.H:\n",
    "            self.H[layer] = {\n",
    "                \"weights\": np.zeros_like(layer.weights),\n",
    "                \"biases\": np.zeros_like(layer.biases)\n",
    "            }\n",
    "\n",
    "        self.H[layer][\"weights\"] += layer.dW ** 2\n",
    "        self.H[layer][\"biases\"] += layer.db ** 2\n",
    "\n",
    "        layer.weights -= self.learning_rate * layer.dW / (np.sqrt(self.H[layer][\"weights\"]) + 1e-7)\n",
    "        layer.biases -= self.learning_rate * layer.db / (np.sqrt(self.H[layer][\"biases\"]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (56000, 784), (56000, 10)\n",
      "Testing data shape: (14000, 784), (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = mnist.data.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
    "y = mnist.target.astype(np.int32).values  # Convert to NumPy array using .values\n",
    "\n",
    "# One-hot encode the labels\n",
    "onehot = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = onehot.fit_transform(y.reshape(-1, 1))  # Now y is a NumPy array\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, shape):\n",
    "        # He initialization: std = sqrt(2 / n), where n is the number of input units\n",
    "        n_in = shape[0]\n",
    "        stddev = np.sqrt(2. / n_in)\n",
    "        return np.random.normal(0, stddev, size=shape)\n",
    "\n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, shape):\n",
    "        # Xavier initialization: std = sqrt(1 / n), where n is the number of input units\n",
    "        n_in = shape[0]\n",
    "        stddev = np.sqrt(1. / n_in)\n",
    "        return np.random.normal(0, stddev, size=shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, initializer):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.initializer = initializer\n",
    "        self.weights = self.initializer.initialize((input_size, output_size))\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = np.dot(X, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output, learning_rate):\n",
    "        d_weights = np.dot(self.X.T, d_output)\n",
    "        d_biases = np.sum(d_output, axis=0, keepdims=True)\n",
    "        d_input = np.dot(d_output, self.weights.T)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.biases -= learning_rate * d_biases\n",
    "        return d_input\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = np.maximum(0, X)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        return d_output * (self.X > 0)\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = 1 / (1 + np.exp(-X))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        return d_output * self.output * (1 - self.output)\n",
    "\n",
    "class SoftmaxWithCrossEntropy:\n",
    "    def forward(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        exp_scores = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.softmax_output = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        log_likelihood = -np.log(self.softmax_output[range(len(y)), y.argmax(axis=1)])\n",
    "        loss = np.sum(log_likelihood) / X.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        grad = self.softmax_output\n",
    "        grad[range(len(self.y)), self.y.argmax(axis=1)] -= 1\n",
    "        return grad / self.y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, layer):\n",
    "        # Placeholder for layer update logic. Will be handled in layer's backward pass.\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
